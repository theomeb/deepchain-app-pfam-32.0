{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17fd1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which deepchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6217767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c01a3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biodatasets import list_datasets, load_dataset\n",
    "from deepchain.models import MLP\n",
    "from deepchain.models.utils import (\n",
    "    confusion_matrix_plot,\n",
    "    dataloader_from_numpy,\n",
    "    model_evaluation_accuracy,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d2888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pfam dataset\n",
    "pfam_dataset = load_dataset(\"pfam-32.0\")\n",
    "X, y = pfam_dataset.to_npy_arrays(input_names=[\"split\"], target_names=[\"family_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c739476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We take only the first 1339083 sequences as we have only their embeddings available.\n"
     ]
    }
   ],
   "source": [
    "# get embeddings and filter on available embeddings\n",
    "embeddings = pfam_dataset.get_embeddings(\"sequence\", \"protbert\", \"mean\")\n",
    "available_embeddings_len = len(embeddings)\n",
    "print(f\"We take only the first {available_embeddings_len} sequences as we have only their embeddings available.\")\n",
    "y = y[0][:available_embeddings_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3bc7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eca97e93",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44faa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9558a931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e421b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53f58392",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_train = embeddings[split == 'train']\n",
    "y_train = y[split == 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0419bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_val = embeddings[split == 'dev']\n",
    "y_val = y[split == 'dev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11cbe393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1086741"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a17041b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126171"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbce9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = np.intersect1d(y_train, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6777ba6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13071 unique classes for family_id.\n"
     ]
    }
   ],
   "source": [
    "# process targets\n",
    "#unique_classes = np.unique(y)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"There are {num_classes} unique classes for family_id.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71532e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_classes = set(unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9aead14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train \n",
    "x_train_generator = (x for (x, y) in zip(emb_train, y_train) if y in subset_classes)\n",
    "y_train_generator = (y for (x, y) in zip(emb_train, y_train) if y in subset_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e11584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "779982ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_train = list(x_train_generator)\n",
    "y_train = list(y_train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37de2acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1064950, 1064950)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f779d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eval \n",
    "x_val_generator = (x for (x, y) in zip(emb_val, y_val) if y in subset_classes)\n",
    "y_val_generator = (y for (x, y) in zip(emb_val, y_val) if y in subset_classes)\n",
    "emb_val = list(x_val_generator)\n",
    "y_val = list(y_val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5fe8895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126171"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e894654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: (1064950,), [ 6015  3243 12995 ... 10421 11242   580], 13071 classes\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "labels = le.fit(unique_classes)\n",
    "targets_train = le.transform(y_train)\n",
    "targets_val = le.transform(y_val)\n",
    "print(f\"Targets: {targets_train.shape}, {targets_train}, {len(labels.classes_)} classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e069504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41b93dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c04c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(data=emb_train, targets=targets_train)\n",
    "val_dataset = MyDataset(data=emb_val, targets=targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df805fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=2048)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75831dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 1024])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66be8de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b83ba8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from deepchain.models.torch_model import TorchModel\n",
    "\n",
    "from pytorch_lightning.metrics.functional import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7decdc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FamilyMLP(TorchModel):\n",
    "    \"\"\"Multi-layer perceptron model.\"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: int = 768, output_shape: int = 1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output = nn.Softmax if output_shape > 1 else nn.Sigmoid\n",
    "        self.loss = F.cross_entropy if output_shape > 1 else F.binary_cross_entropy\n",
    "        self._model = nn.Sequential(\n",
    "            nn.Linear(input_shape, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines forward pass\"\"\"\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x).float()\n",
    "        return self._model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"training_step defined the train loop. It is independent of forward\"\"\"\n",
    "        x, y = batch\n",
    "        y_hat = self._model(x)\n",
    "        y = y.long()\n",
    "        #y = torch.unsqueeze(y, 1)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self._model(x)\n",
    "        y = y.long()\n",
    "        \n",
    "        loss = self.loss(y_hat, y)\n",
    "        \n",
    "        preds = torch.max(y_hat, dim=1)[1]\n",
    "        acc = accuracy(preds, y)\n",
    "        \n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Save entire model with torch\"\"\"\n",
    "        torch.save(self._model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "482b621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = FamilyMLP(input_shape=1024, output_shape=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5bb1989",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-30db9dd1bddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e325e4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FamilyMLP(\n",
       "  (_model): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=13071, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c8b4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp._model = torch.load(\"checkpoint/family_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7fcc00ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | _model | Sequential | 4.9 M \n",
      "--------------------------------------\n",
      "4.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.9 M     Total params\n",
      "19.744    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/deepchain-app-pfam/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/582 [00:00<?, ?it/s]                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/deepchain-app-pfam/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  89%|████████▉ | 520/582 [00:23<00:02, 22.41it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0:  90%|████████▉ | 522/582 [00:23<00:02, 22.36it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  90%|█████████ | 525/582 [00:23<00:02, 22.39it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  91%|█████████ | 528/582 [00:23<00:02, 22.41it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  91%|█████████ | 531/582 [00:23<00:02, 22.44it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  92%|█████████▏| 534/582 [00:23<00:02, 22.46it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  92%|█████████▏| 537/582 [00:23<00:02, 22.48it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  93%|█████████▎| 540/582 [00:23<00:01, 22.51it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  93%|█████████▎| 543/582 [00:24<00:01, 22.39it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  94%|█████████▍| 546/582 [00:24<00:01, 22.41it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  94%|█████████▍| 549/582 [00:24<00:01, 22.44it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  95%|█████████▍| 552/582 [00:24<00:01, 22.46it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  95%|█████████▌| 555/582 [00:24<00:01, 22.48it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  96%|█████████▌| 558/582 [00:24<00:01, 22.51it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  96%|█████████▋| 561/582 [00:24<00:00, 22.53it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  97%|█████████▋| 564/582 [00:25<00:00, 22.55it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  97%|█████████▋| 567/582 [00:25<00:00, 22.43it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  98%|█████████▊| 570/582 [00:25<00:00, 22.45it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  98%|█████████▊| 573/582 [00:25<00:00, 22.47it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  99%|█████████▉| 576/582 [00:25<00:00, 22.49it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0:  99%|█████████▉| 579/582 [00:25<00:00, 22.52it/s, loss=0.748, v_num=5, val_loss=0.729, val_acc=0.852, train_loss=0.753]\n",
      "Epoch 0: 100%|██████████| 582/582 [00:25<00:00, 22.52it/s, loss=0.748, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.794]\n",
      "Epoch 1:  89%|████████▉ | 520/582 [00:22<00:02, 22.79it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  90%|████████▉ | 522/582 [00:22<00:02, 22.78it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  90%|█████████ | 525/582 [00:23<00:02, 22.81it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  91%|█████████ | 528/582 [00:23<00:02, 22.83it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  91%|█████████ | 531/582 [00:23<00:02, 22.86it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  92%|█████████▏| 534/582 [00:23<00:02, 22.72it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  92%|█████████▏| 537/582 [00:23<00:01, 22.75it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  93%|█████████▎| 540/582 [00:23<00:01, 22.77it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  93%|█████████▎| 543/582 [00:23<00:01, 22.79it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  94%|█████████▍| 546/582 [00:23<00:01, 22.81it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  94%|█████████▍| 549/582 [00:24<00:01, 22.84it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  95%|█████████▍| 552/582 [00:24<00:01, 22.86it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  95%|█████████▌| 555/582 [00:24<00:01, 22.88it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  96%|█████████▌| 558/582 [00:24<00:01, 22.90it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  96%|█████████▋| 561/582 [00:24<00:00, 22.77it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  97%|█████████▋| 564/582 [00:24<00:00, 22.79it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  97%|█████████▋| 567/582 [00:24<00:00, 22.81it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  98%|█████████▊| 570/582 [00:24<00:00, 22.83it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  98%|█████████▊| 573/582 [00:25<00:00, 22.85it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  99%|█████████▉| 576/582 [00:25<00:00, 22.87it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1:  99%|█████████▉| 579/582 [00:25<00:00, 22.89it/s, loss=0.714, v_num=5, val_loss=0.727, val_acc=0.854, train_loss=0.734]\n",
      "Epoch 1: 100%|██████████| 582/582 [00:25<00:00, 22.89it/s, loss=0.714, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.742]\n",
      "Epoch 2:  89%|████████▉ | 520/582 [00:22<00:02, 22.69it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2:  90%|████████▉ | 522/582 [00:23<00:02, 22.69it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  90%|█████████ | 525/582 [00:23<00:02, 22.71it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  91%|█████████ | 528/582 [00:23<00:02, 22.73it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  91%|█████████ | 531/582 [00:23<00:02, 22.75it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  92%|█████████▏| 534/582 [00:23<00:02, 22.62it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  92%|█████████▏| 537/582 [00:23<00:01, 22.64it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  93%|█████████▎| 540/582 [00:23<00:01, 22.66it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  93%|█████████▎| 543/582 [00:23<00:01, 22.69it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  94%|█████████▍| 546/582 [00:24<00:01, 22.71it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  94%|█████████▍| 549/582 [00:24<00:01, 22.74it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  95%|█████████▍| 552/582 [00:24<00:01, 22.76it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  95%|█████████▌| 555/582 [00:24<00:01, 22.78it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  96%|█████████▌| 558/582 [00:24<00:01, 22.80it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  96%|█████████▋| 561/582 [00:24<00:00, 22.68it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  97%|█████████▋| 564/582 [00:24<00:00, 22.70it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  97%|█████████▋| 567/582 [00:24<00:00, 22.72it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  98%|█████████▊| 570/582 [00:25<00:00, 22.74it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  98%|█████████▊| 573/582 [00:25<00:00, 22.76it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  99%|█████████▉| 576/582 [00:25<00:00, 22.78it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2:  99%|█████████▉| 579/582 [00:25<00:00, 22.81it/s, loss=0.69, v_num=5, val_loss=0.714, val_acc=0.857, train_loss=0.682]\n",
      "Epoch 2: 100%|██████████| 582/582 [00:25<00:00, 22.66it/s, loss=0.69, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.726]\n",
      "Epoch 3:  89%|████████▉ | 520/582 [00:22<00:02, 22.73it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3:  90%|████████▉ | 522/582 [00:22<00:02, 22.73it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  90%|█████████ | 525/582 [00:23<00:02, 22.75it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  91%|█████████ | 528/582 [00:23<00:02, 22.78it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  91%|█████████ | 531/582 [00:23<00:02, 22.80it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  92%|█████████▏| 534/582 [00:23<00:02, 22.82it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  92%|█████████▏| 537/582 [00:23<00:01, 22.69it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  93%|█████████▎| 540/582 [00:23<00:01, 22.71it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  93%|█████████▎| 543/582 [00:23<00:01, 22.74it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  94%|█████████▍| 546/582 [00:23<00:01, 22.76it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  94%|█████████▍| 549/582 [00:24<00:01, 22.78it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  95%|█████████▍| 552/582 [00:24<00:01, 22.81it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  95%|█████████▌| 555/582 [00:24<00:01, 22.83it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  96%|█████████▌| 558/582 [00:24<00:01, 22.85it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  96%|█████████▋| 561/582 [00:24<00:00, 22.72it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  97%|█████████▋| 564/582 [00:24<00:00, 22.74it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  97%|█████████▋| 567/582 [00:24<00:00, 22.76it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  98%|█████████▊| 570/582 [00:25<00:00, 22.78it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  98%|█████████▊| 573/582 [00:25<00:00, 22.80it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  99%|█████████▉| 576/582 [00:25<00:00, 22.82it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3:  99%|█████████▉| 579/582 [00:25<00:00, 22.84it/s, loss=0.66, v_num=5, val_loss=0.701, val_acc=0.860, train_loss=0.671]\n",
      "Epoch 3: 100%|██████████| 582/582 [00:25<00:00, 22.70it/s, loss=0.66, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.682]\n",
      "Epoch 4:  89%|████████▉ | 520/582 [00:22<00:02, 22.82it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4:  90%|████████▉ | 522/582 [00:22<00:02, 22.81it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  90%|█████████ | 525/582 [00:22<00:02, 22.83it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  91%|█████████ | 528/582 [00:23<00:02, 22.86it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  91%|█████████ | 531/582 [00:23<00:02, 22.88it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  92%|█████████▏| 534/582 [00:23<00:02, 22.90it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  92%|█████████▏| 537/582 [00:23<00:01, 22.77it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  93%|█████████▎| 540/582 [00:23<00:01, 22.79it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  93%|█████████▎| 543/582 [00:23<00:01, 22.81it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  94%|█████████▍| 546/582 [00:23<00:01, 22.83it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  94%|█████████▍| 549/582 [00:24<00:01, 22.85it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  95%|█████████▍| 552/582 [00:24<00:01, 22.87it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  95%|█████████▌| 555/582 [00:24<00:01, 22.90it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  96%|█████████▌| 558/582 [00:24<00:01, 22.76it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  96%|█████████▋| 561/582 [00:24<00:00, 22.78it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  97%|█████████▋| 564/582 [00:24<00:00, 22.81it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  97%|█████████▋| 567/582 [00:24<00:00, 22.83it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  98%|█████████▊| 570/582 [00:24<00:00, 22.85it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  98%|█████████▊| 573/582 [00:25<00:00, 22.87it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  99%|█████████▉| 576/582 [00:25<00:00, 22.89it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4:  99%|█████████▉| 579/582 [00:25<00:00, 22.91it/s, loss=0.639, v_num=5, val_loss=0.691, val_acc=0.862, train_loss=0.664]\n",
      "Epoch 4: 100%|██████████| 582/582 [00:25<00:00, 22.76it/s, loss=0.639, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.685]\n",
      "Epoch 5:  89%|████████▉ | 520/582 [00:22<00:02, 22.81it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5:  90%|████████▉ | 522/582 [00:22<00:02, 22.80it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  90%|█████████ | 525/582 [00:23<00:02, 22.83it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  91%|█████████ | 528/582 [00:23<00:02, 22.85it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  91%|█████████ | 531/582 [00:23<00:02, 22.72it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  92%|█████████▏| 534/582 [00:23<00:02, 22.74it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  92%|█████████▏| 537/582 [00:23<00:01, 22.76it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  93%|█████████▎| 540/582 [00:23<00:01, 22.79it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  93%|█████████▎| 543/582 [00:23<00:01, 22.81it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  94%|█████████▍| 546/582 [00:23<00:01, 22.83it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  94%|█████████▍| 549/582 [00:24<00:01, 22.86it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  95%|█████████▍| 552/582 [00:24<00:01, 22.88it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  95%|█████████▌| 555/582 [00:24<00:01, 22.75it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  96%|█████████▌| 558/582 [00:24<00:01, 22.77it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  96%|█████████▋| 561/582 [00:24<00:00, 22.79it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  97%|█████████▋| 564/582 [00:24<00:00, 22.82it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  97%|█████████▋| 567/582 [00:24<00:00, 22.84it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  98%|█████████▊| 570/582 [00:24<00:00, 22.86it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  98%|█████████▊| 573/582 [00:25<00:00, 22.74it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  99%|█████████▉| 576/582 [00:25<00:00, 22.76it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5:  99%|█████████▉| 579/582 [00:25<00:00, 22.78it/s, loss=0.616, v_num=5, val_loss=0.684, val_acc=0.864, train_loss=0.620]\n",
      "Epoch 5: 100%|██████████| 582/582 [00:25<00:00, 22.78it/s, loss=0.616, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.620]\n",
      "Epoch 6:  89%|████████▉ | 520/582 [00:22<00:02, 22.66it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6:  90%|████████▉ | 522/582 [00:23<00:02, 22.66it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  90%|█████████ | 525/582 [00:23<00:02, 22.68it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  91%|█████████ | 528/582 [00:23<00:02, 22.71it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  91%|█████████ | 531/582 [00:23<00:02, 22.73it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  92%|█████████▏| 534/582 [00:23<00:02, 22.75it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  92%|█████████▏| 537/582 [00:23<00:01, 22.78it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  93%|█████████▎| 540/582 [00:23<00:01, 22.80it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  93%|█████████▎| 543/582 [00:23<00:01, 22.82it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  94%|█████████▍| 546/582 [00:23<00:01, 22.84it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  94%|█████████▍| 549/582 [00:24<00:01, 22.87it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  95%|█████████▍| 552/582 [00:24<00:01, 22.89it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  95%|█████████▌| 555/582 [00:24<00:01, 22.91it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  96%|█████████▌| 558/582 [00:24<00:01, 22.78it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  96%|█████████▋| 561/582 [00:24<00:00, 22.80it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  97%|█████████▋| 564/582 [00:24<00:00, 22.82it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  97%|█████████▋| 567/582 [00:24<00:00, 22.84it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  98%|█████████▊| 570/582 [00:24<00:00, 22.86it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  98%|█████████▊| 573/582 [00:25<00:00, 22.88it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  99%|█████████▉| 576/582 [00:25<00:00, 22.90it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6:  99%|█████████▉| 579/582 [00:25<00:00, 22.92it/s, loss=0.594, v_num=5, val_loss=0.674, val_acc=0.866, train_loss=0.574]\n",
      "Epoch 6: 100%|██████████| 582/582 [00:25<00:00, 22.92it/s, loss=0.594, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.607]\n",
      "Epoch 7:  89%|████████▉ | 520/582 [00:23<00:02, 22.59it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7:  90%|████████▉ | 522/582 [00:23<00:02, 22.58it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  90%|█████████ | 525/582 [00:23<00:02, 22.61it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  91%|█████████ | 528/582 [00:23<00:02, 22.63it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  91%|█████████ | 531/582 [00:23<00:02, 22.66it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  92%|█████████▏| 534/582 [00:23<00:02, 22.68it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  92%|█████████▏| 537/582 [00:23<00:01, 22.71it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  93%|█████████▎| 540/582 [00:23<00:01, 22.58it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  93%|█████████▎| 543/582 [00:24<00:01, 22.61it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  94%|█████████▍| 546/582 [00:24<00:01, 22.63it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  94%|█████████▍| 549/582 [00:24<00:01, 22.65it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  95%|█████████▍| 552/582 [00:24<00:01, 22.68it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  95%|█████████▌| 555/582 [00:24<00:01, 22.70it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  96%|█████████▌| 558/582 [00:24<00:01, 22.72it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  96%|█████████▋| 561/582 [00:24<00:00, 22.75it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  97%|█████████▋| 564/582 [00:24<00:00, 22.62it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  97%|█████████▋| 567/582 [00:25<00:00, 22.64it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  98%|█████████▊| 570/582 [00:25<00:00, 22.66it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  98%|█████████▊| 573/582 [00:25<00:00, 22.68it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  99%|█████████▉| 576/582 [00:25<00:00, 22.70it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7:  99%|█████████▉| 579/582 [00:25<00:00, 22.72it/s, loss=0.58, v_num=5, val_loss=0.667, val_acc=0.867, train_loss=0.587]\n",
      "Epoch 7: 100%|██████████| 582/582 [00:25<00:00, 22.58it/s, loss=0.58, v_num=5, val_loss=0.663, val_acc=0.868, train_loss=0.597]\n",
      "Epoch 8:  22%|██▏       | 130/582 [00:05<00:19, 23.49it/s, loss=0.556, v_num=5, val_loss=0.663, val_acc=0.868, train_loss=0.547]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/deepchain-app-pfam/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "mlp.fit(train_dataloader, test_dataloader, epochs=10, auto_lr_find=True, auto_scale_batch_size=True, gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5845383",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.save_model(\"family_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75fe8597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/deepchain-app-pfam-32.0\n",
      "Epoch 8:  22%|██▏       | 130/582 [00:23<01:20,  5.60it/s, loss=0.556, v_num=5, val_loss=0.663, val_acc=0.868, train_loss=0.547]"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53b3bb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1048])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(mlp(next(iter(train_dataloader))[0]), 1)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "486026df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bff2c444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(mlp(x), 1)[1] == y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0101d9c",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d27b27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "363aa8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation_accuracy(\n",
    "    dataloader: DataLoader, model\n",
    ") -> Tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Make prediction for test data\n",
    "    Args:\n",
    "        dataloader: a torch dataloader containing dataset to be evaluated\n",
    "        model : a callable trained model with a predict method\n",
    "    \"\"\"\n",
    "    prediction, truth = [], []\n",
    "    for X, y in dataloader:\n",
    "        y_hat = torch.max(model.predict(X), 1)[1]\n",
    "        prediction += y_hat\n",
    "        truth += y.detach().numpy().flatten().tolist()\n",
    "\n",
    "    prediction, truth = np.array(prediction), np.array(truth)\n",
    "\n",
    "    acc_score = accuracy_score(truth, prediction)\n",
    "    print(f\" Test :  accuracy score : {acc_score:0.2f}\")\n",
    "\n",
    "    return prediction, truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87ee1a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test :  accuracy score : 0.83\n"
     ]
    }
   ],
   "source": [
    "prediction, truth = model_evaluation_accuracy(train_dataloader, mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a05e82e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test :  accuracy score : 0.81\n"
     ]
    }
   ],
   "source": [
    "prediction, truth = model_evaluation_accuracy(test_dataloader, mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ce608",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29abeba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d783b861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(le, 'label_encoder.joblib')\n",
    "label_encoder = joblib.load('label_encoder.joblib')\n",
    "label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "6e20e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(sequences: List[str]):\n",
    "    \"\"\"Return a list of all proteins score\"\"\"\n",
    "\n",
    "    #x_embedding = self.transformer.compute_embeddings(sequences)[\"mean\"]\n",
    "    x_embedding = embeddings[:len(sequences)]\n",
    "    \n",
    "    y_hat = mlp(torch.tensor(x_embedding))\n",
    "    preds = torch.max(y_hat, dim=1)[1]\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    \n",
    "    family_preds = label_encoder.inverse_transform(preds)\n",
    "\n",
    "    family_list = [{\"family_id\": family_pred} for family_pred in family_preds]\n",
    "\n",
    "    return family_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "c9eac8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'family_id': 'GMC_oxred_C'}, {'family_id': 'DUF2887'}]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [\n",
    "        \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\",\n",
    "        \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\",\n",
    "    ]\n",
    "compute_scores(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "2c9c7e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1a9495f9c5fc493c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1a9495f9c5fc493c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6009;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da5c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m68"
  },
  "kernelspec": {
   "display_name": "deepchain-app-pfam",
   "language": "python",
   "name": "deepchain-app-pfam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
